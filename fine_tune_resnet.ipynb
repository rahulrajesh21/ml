{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Fine-Tuning ResNet-50 for Meeting Context Classification\n",
                "\n",
                "This notebook guides you through the process of fine-tuning a pre-trained ResNet-50 model to distinguish between:\n",
                "1.  **Slides/Screen Share** (High informational content)\n",
                "2.  **People/Camera** (Social/Interactional content)\n",
                "\n",
                "**Environment**: Kaggle (LFW dataset present in `../input/lfw-dataset/`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import random\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "from PIL import Image\n",
                "\n",
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from torchvision import models, transforms\n",
                "\n",
                "# Check device\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Using device: {device}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup LFW Dataset (People)\n",
                "Using the pre-loaded LFW dataset from Kaggle input."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define Paths\n",
                "lfw_root = \"../input/lfw-dataset/lfw-deepfunneled/lfw-deepfunneled/\"\n",
                "csv_root = \"../input/lfw-dataset/\"\n",
                "\n",
                "# Load CSVs (User provided logic)\n",
                "peopleDevTrain = pd.read_csv(os.path.join(csv_root, \"peopleDevTrain.csv\"))\n",
                "peopleDevTest = pd.read_csv(os.path.join(csv_root, \"peopleDevTest.csv\"))\n",
                "\n",
                "print(f\"Train People: {len(peopleDevTrain)}\")\n",
                "print(f\"Test People: {len(peopleDevTest)}\")\n",
                "\n",
                "# Helper to get all image paths for a person\n",
                "def get_person_image_paths(person_name, image_count):\n",
                "    paths = []\n",
                "    person_dir = os.path.join(lfw_root, person_name)\n",
                "    # Filename format: name_0001.jpg\n",
                "    for i in range(1, image_count + 1):\n",
                "        filename = f\"{person_name}_{i:04d}.jpg\"\n",
                "        paths.append(os.path.join(person_dir, filename))\n",
                "    return paths\n",
                "\n",
                "# Collect all paths\n",
                "train_person_paths = []\n",
                "for _, row in peopleDevTrain.iterrows():\n",
                "    train_person_paths.extend(get_person_image_paths(row['name'], row['images']))\n",
                "\n",
                "test_person_paths = []\n",
                "for _, row in peopleDevTest.iterrows():\n",
                "    test_person_paths.extend(get_person_image_paths(row['name'], row['images']))\n",
                "\n",
                "print(f\"Total Train Person Images: {len(train_person_paths)}\")\n",
                "print(f\"Total Test Person Images: {len(test_person_paths)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Setup SlideAudit Dataset (Slides)\n",
                "Cloning the SlideAudit dataset from GitHub."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import subprocess\n",
                "import shutil\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "slide_root = \"./slide_dataset\" # Writable directory\n",
                "slide_images_dir = os.path.join(slide_root, \"SlideAudit\", \"data\", \"images\")\n",
                "slide_repo = \"https://github.com/zhuohaouw/SlideAudit.git\"\n",
                "\n",
                "if not os.path.exists(slide_images_dir):\n",
                "    print(\"Cloning SlideAudit dataset...\")\n",
                "    if os.path.exists(slide_root):\n",
                "        shutil.rmtree(slide_root)\n",
                "    os.makedirs(slide_root, exist_ok=True)\n",
                "    subprocess.run([\"git\", \"clone\", slide_repo, os.path.join(slide_root, \"SlideAudit\")], check=True)\n",
                "    print(\"SlideAudit setup complete.\")\n",
                "else:\n",
                "    print(\"SlideAudit dataset already exists.\")\n",
                "\n",
                "# Collect all slide image paths\n",
                "all_slide_paths = []\n",
                "for img in os.listdir(slide_images_dir):\n",
                "    if img.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
                "        all_slide_paths.append(os.path.join(slide_images_dir, img))\n",
                "\n",
                "# Split Slides into Train/Test (70/30 split)\n",
                "train_slide_paths, test_slide_paths = train_test_split(all_slide_paths, test_size=0.3, random_state=42)\n",
                "\n",
                "print(f\"Total Train Slide Images: {len(train_slide_paths)}\")\n",
                "print(f\"Total Test Slide Images: {len(test_slide_paths)}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Handling \"Picture-in-Picture\" (Mixed Contexts)\n",
                "\n",
                "**Problem**: Real meetings often show a slide WITH a small webcam overlay (Picture-in-Picture). We want the model to classify these as **Slides** because the slide is the dominant content.\n",
                "\n",
                "**Solution**: We will synthetically generate \"PiP\" images by randomly pasting small person images onto slide images and adding them to the training set labeled as 'Slide'."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def generate_pip_samples(slide_paths, person_paths, output_dir, num_samples=500):\n",
                "    os.makedirs(output_dir, exist_ok=True)\n",
                "    pip_paths = []\n",
                "    \n",
                "    print(f\"Generating {num_samples} synthetic Picture-in-Picture samples...\")\n",
                "    \n",
                "    for i in range(num_samples):\n",
                "        # 1. Pick random slide and person\n",
                "        slide_path = random.choice(slide_paths)\n",
                "        person_path = random.choice(person_paths)\n",
                "        \n",
                "        try:\n",
                "            slide_img = Image.open(slide_path).convert('RGB')\n",
                "            person_img = Image.open(person_path).convert('RGB')\n",
                "            \n",
                "            # 2. Resize person to be smaller (e.g., 20-30% of slide width)\n",
                "            scale = random.uniform(0.2, 0.3)\n",
                "            new_width = int(slide_img.width * scale)\n",
                "            aspect_ratio = person_img.height / person_img.width\n",
                "            new_height = int(new_width * aspect_ratio)\n",
                "            person_img = person_img.resize((new_width, new_height))\n",
                "            \n",
                "            # 3. Paste in a random corner\n",
                "            # Corners: Top-Right, Bottom-Right, Top-Left, Bottom-Left\n",
                "            corners = [\n",
                "                (slide_img.width - new_width - 10, 10), # TR\n",
                "                (slide_img.width - new_width - 10, slide_img.height - new_height - 10), # BR\n",
                "                (10, 10), # TL\n",
                "                (10, slide_img.height - new_height - 10) # BL\n",
                "            ]\n",
                "            pos = random.choice(corners)\n",
                "            \n",
                "            slide_img.paste(person_img, pos)\n",
                "            \n",
                "            # 4. Save\n",
                "            save_path = os.path.join(output_dir, f\"pip_{i}.jpg\")\n",
                "            slide_img.save(save_path)\n",
                "            pip_paths.append(save_path)\n",
                "            \n",
                "        except Exception as e:\n",
                "            print(f\"Error generating PiP sample: {e}\")\n",
                "            \n",
                "    return pip_paths\n",
                "\n",
                "# Generate PiP samples for Training only\n",
                "pip_train_dir = \"./pip_dataset/train\"\n",
                "train_pip_paths = generate_pip_samples(train_slide_paths, train_person_paths, pip_train_dir, num_samples=1000)\n",
                "\n",
                "# Add these to our training list (Label = 1 for Slide)\n",
                "print(f\"Added {len(train_pip_paths)} PiP images to training set.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Custom Dataset Class\n",
                "Updated to include the PiP paths."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MeetingContextDataset(Dataset):\n",
                "    def __init__(self, person_paths, slide_paths, pip_paths=[], transform=None):\n",
                "        self.person_paths = person_paths\n",
                "        self.slide_paths = slide_paths\n",
                "        self.pip_paths = pip_paths\n",
                "        self.transform = transform\n",
                "        \n",
                "        # 0 = Person, 1 = Slide\n",
                "        self.data = []\n",
                "        for p in person_paths:\n",
                "            self.data.append((p, 0))\n",
                "        for p in slide_paths:\n",
                "            self.data.append((p, 1))\n",
                "        for p in pip_paths:\n",
                "            self.data.append((p, 1)) # PiP counts as Slide\n",
                "            \n",
                "    def __len__(self):\n",
                "        return len(self.data)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        img_path, label = self.data[idx]\n",
                "        try:\n",
                "            image = Image.open(img_path).convert('RGB')\n",
                "            if self.transform:\n",
                "                image = self.transform(image)\n",
                "            return image, label\n",
                "        except Exception as e:\n",
                "            print(f\"Error loading {img_path}: {e}\")\n",
                "            return torch.zeros((3, 224, 224)), label\n",
                "\n",
                "# Transforms\n",
                "data_transforms = {\n",
                "    'train': transforms.Compose([\n",
                "        transforms.RandomResizedCrop(224),\n",
                "        transforms.RandomHorizontalFlip(),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "    ]),\n",
                "    'val': transforms.Compose([\n",
                "        transforms.Resize(256),\n",
                "        transforms.CenterCrop(224),\n",
                "        transforms.ToTensor(),\n",
                "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
                "    ]),\n",
                "}\n",
                "\n",
                "# Create Datasets (Include PiP in training)\n",
                "train_dataset = MeetingContextDataset(train_person_paths, train_slide_paths, train_pip_paths, data_transforms['train'])\n",
                "val_dataset = MeetingContextDataset(test_person_paths, test_slide_paths, [], data_transforms['val'])\n",
                "\n",
                "dataloaders = {\n",
                "    'train': DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=2),\n",
                "    'val': DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
                "}\n",
                "\n",
                "dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n",
                "print(f\"Dataset Sizes: {dataset_sizes}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Train ResNet-50 with Class Balancing\n",
                "We calculate class weights to handle the imbalance between Person (many) and Slide (fewer) images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Calculate Class Weights\n",
                "count_person = len(train_person_paths)\n",
                "count_slide = len(train_slide_paths) + len(train_pip_paths)\n",
                "total_samples = count_person + count_slide\n",
                "\n",
                "print(f\"Training Samples - Person: {count_person}, Slide: {count_slide}\")\n",
                "\n",
                "# Inverse frequency weights\n",
                "weight_person = 1.0 / count_person\n",
                "weight_slide = 1.0 / count_slide\n",
                "\n",
                "# Normalize\n",
                "norm_factor = (weight_person + weight_slide) / 2.0\n",
                "weight_person /= norm_factor\n",
                "weight_slide /= norm_factor\n",
                "\n",
                "class_weights = torch.tensor([weight_person, weight_slide]).to(device)\n",
                "print(f\"Class Weights: Person={weight_person:.4f}, Slide={weight_slide:.4f}\")\n",
                "\n",
                "# Initialize Model\n",
                "model = models.resnet50(pretrained=True)\n",
                "\n",
                "# Freeze layers (optional)\n",
                "for param in model.parameters():\n",
                "    param.requires_grad = False\n",
                "\n",
                "# Modify Output Layer (2 classes: Person, Slide)\n",
                "num_ftrs = model.fc.in_features\n",
                "model.fc = nn.Linear(num_ftrs, 2)\n",
                "model = model.to(device)\n",
                "\n",
                "# Use Weighted Loss\n",
                "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
                "optimizer = optim.SGD(model.fc.parameters(), lr=0.001, momentum=0.9)\n",
                "\n",
                "# Training Loop\n",
                "def train_model(model, criterion, optimizer, num_epochs=5):\n",
                "    since = time.time()\n",
                "    best_acc = 0.0\n",
                "    best_model_wts = copy.deepcopy(model.state_dict())\n",
                "\n",
                "    for epoch in range(num_epochs):\n",
                "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
                "        print('-' * 10)\n",
                "\n",
                "        for phase in ['train', 'val']:\n",
                "            if phase == 'train':\n",
                "                model.train()\n",
                "            else:\n",
                "                model.eval()\n",
                "\n",
                "            running_loss = 0.0\n",
                "            running_corrects = 0\n",
                "\n",
                "            for inputs, labels in dataloaders[phase]:\n",
                "                inputs = inputs.to(device)\n",
                "                labels = labels.to(device)\n",
                "\n",
                "                optimizer.zero_grad()\n",
                "\n",
                "                with torch.set_grad_enabled(phase == 'train'):\n",
                "                    outputs = model(inputs)\n",
                "                    _, preds = torch.max(outputs, 1)\n",
                "                    loss = criterion(outputs, labels)\n",
                "\n",
                "                    if phase == 'train':\n",
                "                        loss.backward()\n",
                "                        optimizer.step()\n",
                "\n",
                "                running_loss += loss.item() * inputs.size(0)\n",
                "                running_corrects += torch.sum(preds == labels.data)\n",
                "\n",
                "            epoch_loss = running_loss / dataset_sizes[phase]\n",
                "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
                "\n",
                "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
                "\n",
                "            if phase == 'val' and epoch_acc > best_acc:\n",
                "                best_acc = epoch_acc\n",
                "                best_model_wts = copy.deepcopy(model.state_dict())\n",
                "\n",
                "    time_elapsed = time.time() - since\n",
                "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
                "    print(f'Best val Acc: {best_acc:4f}')\n",
                "    model.load_state_dict(best_model_wts)\n",
                "    return model\n",
                "\n",
                "model = train_model(model, criterion, optimizer, num_epochs=5)\n",
                "\n",
                "# Save Model\n",
                "save_path = \"resnet50_meeting_context.pth\"\n",
                "torch.save(model.state_dict(), save_path)\n",
                "print(f\"Model saved to {save_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Evaluation Metrics\n",
                "We will evaluate the model on the validation set to calculate:\n",
                "- Confusion Matrix (TP, FP, TN, FN)\n",
                "- Precision, Recall, F1-Score"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "import seaborn as sns\n",
                "\n",
                "def evaluate_model(model, dataloader):\n",
                "    model.eval()\n",
                "    all_preds = []\n",
                "    all_labels = []\n",
                "    \n",
                "    print(\"Running evaluation on validation set...\")\n",
                "    with torch.no_grad():\n",
                "        for inputs, labels in dataloader:\n",
                "            inputs = inputs.to(device)\n",
                "            labels = labels.to(device)\n",
                "            \n",
                "            outputs = model(inputs)\n",
                "            _, preds = torch.max(outputs, 1)\n",
                "            \n",
                "            all_preds.extend(preds.cpu().numpy())\n",
                "            all_labels.extend(labels.cpu().numpy())\n",
                "            \n",
                "    # Confusion Matrix\n",
                "    # Labels: 0 = Person, 1 = Slide\n",
                "    cm = confusion_matrix(all_labels, all_preds)\n",
                "    tn, fp, fn, tp = cm.ravel()\n",
                "    \n",
                "    print(\"\\n--- Confusion Matrix ---\")\n",
                "    print(f\"True Negatives (Person correctly identified): {tn}\")\n",
                "    print(f\"False Positives (Person identified as Slide): {fp}\")\n",
                "    print(f\"False Negatives (Slide identified as Person): {fn}\")\n",
                "    print(f\"True Positives (Slide correctly identified): {tp}\")\n",
                "    \n",
                "    # Plotting\n",
                "    plt.figure(figsize=(6,5))\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Person', 'Slide'], yticklabels=['Person', 'Slide'])\n",
                "    plt.ylabel('Actual')\n",
                "    plt.xlabel('Predicted')\n",
                "    plt.title('Confusion Matrix')\n",
                "    plt.show()\n",
                "    \n",
                "    # Classification Report\n",
                "    print(\"\\n--- Classification Report ---\")\n",
                "    print(classification_report(all_labels, all_preds, target_names=['Person', 'Slide']))\n",
                "\n",
                "# Run Evaluation\n",
                "evaluate_model(model, dataloaders['val'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Visual Inference (Sanity Check)\n",
                "Let's see the model in action by visualizing predictions on random validation images."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def visualize_predictions(model, dataloader, num_images=6):\n",
                "    model.eval()\n",
                "    images_so_far = 0\n",
                "    plt.figure(figsize=(15, 10))\n",
                "    \n",
                "    class_names = ['Person', 'Slide']\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for i, (inputs, labels) in enumerate(dataloader):\n",
                "            inputs = inputs.to(device)\n",
                "            labels = labels.to(device)\n",
                "            \n",
                "            outputs = model(inputs)\n",
                "            _, preds = torch.max(outputs, 1)\n",
                "            \n",
                "            for j in range(inputs.size()[0]):\n",
                "                images_so_far += 1\n",
                "                ax = plt.subplot(num_images // 3 + 1, 3, images_so_far)\n",
                "                ax.axis('off')\n",
                "                \n",
                "                # Un-normalize image for display\n",
                "                img = inputs.cpu().data[j].numpy().transpose((1, 2, 0))\n",
                "                mean = np.array([0.485, 0.456, 0.406])\n",
                "                std = np.array([0.229, 0.224, 0.225])\n",
                "                img = std * img + mean\n",
                "                img = np.clip(img, 0, 1)\n",
                "                \n",
                "                true_label = class_names[labels[j]]\n",
                "                pred_label = class_names[preds[j]]\n",
                "                \n",
                "                color = 'green' if true_label == pred_label else 'red'\n",
                "                ax.set_title(f'True: {true_label} | Pred: {pred_label}', color=color)\n",
                "                plt.imshow(img)\n",
                "                \n",
                "                if images_so_far == num_images:\n",
                "                    return\n",
                "\n",
                "visualize_predictions(model, dataloaders['val'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Download the Model\n",
                "\n",
                "**How to download the `.pth` file from Kaggle:**\n",
                "\n",
                "1.  Look at the **Output** section on the right sidebar of the Kaggle notebook editor.\n",
                "2.  You should see `resnet50_meeting_context.pth` listed under `/kaggle/working`.\n",
                "3.  Click the **three dots (...)** next to the file name.\n",
                "4.  Select **Download**.\n",
                "\n",
                "Once downloaded, place this file in your local project directory (e.g., inside `ml/models/`) so your RoME app can load it."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}